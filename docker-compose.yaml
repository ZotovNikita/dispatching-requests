services:
  ollama:
    volumes:
      - ${LOCAL_OLLAMA_MODELS}:/opt/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    image: ollama/ollama:latest
    ports:
      - ${OLLAMA_PORT}:11434
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=/opt/.ollama
      - OLLAMA_ORIGINS=*
    networks:
      - sila-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  centrifugo:
    image: centrifugo/centrifugo:v5.4.7
    container_name: centrifugo
    restart: always
    volumes:
      - ./cent_config.json:/centrifugo/config.json
    command: centrifugo -c config.json
    ports:
      - ${CENTRIFUGO_PORT}:8000
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
    networks:
      - sila-net

  backend:
    build:
      context: .
      dockerfile: ./Dockerfile
    container_name: backend
    restart: always
    volumes:
      - ./models:/backend/models
      - ${LOCAL_HUGGINGFACE_MODELS}:/opt/.cache/huggingface
    ports:
      - ${BACKEND_PORT}:${BACKEND_PORT}
    environment:
      - HF_HOME=/opt/.cache/huggingface
      - APP__TITLE=${BACKEND_TITLE}
      - APP__HOST=${BACKEND_HOST}
      - APP__PORT=${BACKEND_PORT}
      - CENTRIFUGO__API_URL=${CENTRIFUGO_API_URL}
      - CENTRIFUGO__API_KEY=${CENTRIFUGO_API_KEY}
      - LLM_MODEL_URL=${OLLAMA_API_URL}
    env_file:
      - .env
    depends_on:
      - ollama
      - centrifugo
    networks:
      - sila-net

  streamlit:
    build:
      context: .
      dockerfile: ./Dockerfile.streamlit
    container_name: streamlit
    restart: always
    ports:
      - ${FRONTEND_PORT}:8501
    env_file:
      - .env
    depends_on:
      - backend
    networks:
      - sila-net

networks:
  sila-net:
    driver: bridge
